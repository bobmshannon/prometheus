default_conf:
  ENV:
    CONFIG_FILE: '{{ service_home }}/var/conf/prometheus.yml'
    LOG_FILE: '{{ service_home }}/var/log/service.log'
    LOG_LEVEL: info
    RELOAD_URL: '{{ conf.prometheus.protocol }}://{{ conf.prometheus.hostname }}:{{ conf.prometheus.port }}/-/reload'
    STORAGE_TSDB_PATH: '{{ service_home }}/var/data/'
    WEB_CA_FILE: '{{ conf.prometheus.ca_file }}'
    WEB_CERTIFICATE_FILE: '{{ conf.prometheus.certificate_file }}'
    WEB_EXTERNAL_URL: '{{ self_discovered.prometheus.external_uri }}'
    WEB_KEY_FILE: '{{ conf.prometheus.key_file }}'
    WEB_LISTEN_ADDRESS: '{{ conf.prometheus.address }}:{{ conf.prometheus.port }}'
    WEB_PROTOCOL: '{{ conf.prometheus.protocol }}'
    WEB_ROUTE_PREFIX: '{{ conf.prometheus.context_path }}'
  logrotate:
    files:
      - filepath: '{{ conf.ENV.LOG_FILE }}'
        settings: []
    global:
      - compress
      - copytruncate
      - dateext
      - daily
      - rotate 7
      - size 512M
  oauth:
    client_id: '{{stack_name}}-{{service_name}}'
    client_secret: "{{plaintext_secret('oauth-secret')}}"
  user_defined_rules:
    groups: []
  baseline_rules:
    groups:
    ##
    ## Rules that evaluate the health of Prometheus.
    ##
    - name: prometheus.rules
      rules:
      ## Alert if live-reloading the Prometheus configuration failed.
      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 10m
        labels:
          severity: p1
        annotations:
          description: "{{`The last configuration reload of Prometheus ({{ $label.instance }}) failed. This indicates that the configuration file on disk is invalid and that Prometheus will fail to start successfully if restarted.`}}"
          summary: "{{`Live-reload of Prometheus configuration failed`}}"
      ## Alert if the Prometheus notification queue is running full.
      - alert: PrometheusNotificationQueueRunningFull
        expr: predict_linear(prometheus_notifications_queue_length[5m], 60 * 30) > prometheus_notifications_queue_capacity
        for: 10m
        labels:
          severity: p1
        annotations:
          description: "{{`The Prometheus alert notification queue is running full for {{ $labels.instance }}`}}"
          summary: "Prometheus alert notification queue is running full"
      ## Alert if there are errors while sending alerts to Alertmanager.
      - alert: PrometheusErrorSendingAlerts
        expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m]) > 0.01
        for: 10m
        labels:
          severity: p1
        annotations:
          description: "{{`Errors while sending alerts from Prometheus ({{ $labels.instance }}) to Alertmanager {{$labels.Alertmanager}}`}}"
          summary: "Errors while sending alert from Prometheus"
      ## Page if there is an excessively high number of errors while sending alerts to Alertmanager.
      - alert: PrometheusErrorSendingAlerts
        expr: rate(prometheus_notifications_errors_total[5m]) / rate(prometheus_notifications_sent_total[5m]) > 0.03
        for: 10m
        labels:
          severity: p0
        annotations:
          description: "{{`Errors while sending alerts from Prometheus ({{ $labels.instance }}) to Alertmanager {{$labels.Alertmanager}}`}}"
          summary: "Errors while sending alerts from Prometheus"
      ## Alert if Prometheus is not connected to any Alertmanagers.
      - alert: PrometheusNotConnectedToAlertmanagers
        expr: prometheus_notifications_alertmanagers_discovered < 1
        for: 10m
        labels:
          severity: p1
        annotations:
          description: "{{`Prometheus ({{ $labels.instance }}) is not connected to any Alertmanagers`}}"
          summary: "Prometheus is not connected to any Alertmanagers"
      ## Alert if there TSDB reload failures.
      - alert: PrometheusTSDBReloadsFailing
        expr: increase(prometheus_tsdb_reloads_failures_total[2h]) > 0
        for: 12h
        labels:
          severity: p1
        annotations:
          description: "{{`{{ $labels.job }} at {{ $labels.instance }} had {{ $value | humanize }} reload failures over the last two hours.`}}"
          summary: "Prometheus has issues reloading data blocks from disk"
      ## Alert if there are TSDB compaction failures.
      - alert: PrometheusTSDBCompactionsFailing
        expr: increase(prometheus_tsdb_compactions_failed_total[2h]) > 0
        for: 12h
        labels:
          severity: p1
        annotations:
          description: "{{`{{ $labels.job }} at {{ $labels.instance }} had {{ $value | humanize }} compaction failures over the last two hours.`}}"
          summary: "Prometheus has issues compacting sample blocks"
      ## Alert if the Prometheus WAL is corrupted.
      - alert: PrometheusTSDBWALCorruptions
        expr: prometheus_tsdb_wal_corruptions_total > 0
        for: 4h
        labels:
          severity: p1
        annotations:
          description: "{{`{{$labels.job}} at {{$labels.instance}} has a corrupted write-ahead log (WAL).`}}"
          summary: "Prometheus write-ahead log is corrupted"
      ## Alert if Prometheus is not ingesting any samples.
      - alert: PrometheusNotIngestingSamples
        expr: rate(prometheus_tsdb_head_samples_appended_total[5m]) <= 0
        for: 10m
        labels:
          severity: p0
        annotations:
          description: "{{`Prometheus ({{ $labels.instance }} isn't ingesting samples.`}}"
          summary: "Prometheus isn't ingesting samples"
      ## Alert if Prometheus is ingesting duplicate samples.
      - alert: PrometheusTargetScrapesDuplicate
        expr: increase(prometheus_target_scrapes_sample_duplicate_timestamp_total[5m]) > 0
        for: 10m
        labels:
          severity: p1
        annotations:
          description: "{{`Prometheus ({{ $labels.instance }}) has many samples rejected due to duplicate timestamps but different values.`}}"
          summary: "Prometheus has many samples rejected"
    ##
    ## Rules that evaluate the health of Alertmanager.
    ##
    - name: alertmanager.rules
      rules:
      # Alert if Alertmanager is down.
      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 10m
        labels:
          severity: p1
        annotations:
          description: Prometheus failed to scrape Alertmanager which indicates that it is down or in an unhealthy state.
          summary: Alertmanager down or missing
      # Alert if live-reloading the Alertmanager configuration failed.
      - alert: AlertmanagerConfigReloadFailed
        expr: alertmanager_config_last_reload_successful == 0
        for: 10m
        labels:
          severity: p1
        annotations:
          description: "{{`Reloading Alertmanager's configuration has failed for {{ $labels.instance }}.`}}"
          summary: "Alertmanager's configuration reload failed"
    ##
    ## Rules that evaluate the health of the Sink.
    ##
    - name: sink.rules
      rules:
      # Alert if the sink stops processing metrics.
      - alert: SinkNotIngestingMetrics
        expr: rate(haruspex_sink_metrics_processed:count{}[5m]) == 0
        for: 10m
        labels:
          severity: p1
        annotations:
          description: "{{`The Sink installed on {{ $labels.host }} has stopped ingesting {{ $labels.input }} metrics. This indicates that metrics agents have stopped sending telemetry, or that the Sink is in an unhealthy state.`}}"
          summary: "{{`Sink not ingesting {{ $labels.input }} metrics`}}"
      # Alert if the sink stops emitting telemetry.
      - alert: SinkNoData
        expr: absent(haruspex_sink_metrics_processed:count) == 1
        for: 10m
        labels:
          severity: p1
        annotations:
          description: "The Sink installed on {{ discovered.haruspex-sink.hostname }} has stopped emitting metrics. This indicates that the Sink is not running, is in an unhealthy state, or that metrics agents (e.g. caelumd) are no longer functioning correctly."
          summary: "Sink installed on {{ discovered.haruspex-sink.hostname }} stopped emitting metrics"
    ##
    ## The set of SLS compliant service baseline alerting rules evaluated by Prometheus.
    ##
    - name: sls.rules
      rules:
      # Page if a sls-service reports an non-healthy state.
      - alert: SLSServiceNotHealthy
        expr: haruspex_slshealthstatus_value != 200
        for: 5m
        labels:
          severity: p0
        annotations:
          description: "{{`The SLS Service {{ $labels.product }} on host {{ $labels.host }} is reporting a non-healthy state {{ $labels.state }}{{ if $labels.type }} for the check type {{ $labels.type }}{{ end }}. To see the entire check result message, please login to Gemini and view the service's health status information."
          summary: "{{`{{ $labels.product }} is reporting {{ $labels.state }}`}}"
    ##
    ## The set of system baseline rules evaluated by Prometheus.
    ##
    - name: host.rules
      rules:
      ## Recording rules that pre-compute frequently needed and computationally expensive expressions.
      # CPU usage percent
      - record: node_cpu_usage_percent
        expr: avg by (mode, instance) (100*rate(node_cpu_seconds_total{job="node"}[5m]))
      # Memory percent available for application use
      - record: node_memory_available_percent
        expr: 100*(node_memory_MemFree_bytes+node_memory_Cached_bytes+node_memory_Buffers_bytes)/node_memory_MemTotal_bytes
      # Filesystem space usage percent
      - record: node_filesystem_usage_percent
        expr: 100*(1-node_filesystem_avail_bytes{mountpoint!="/media"}/node_filesystem_size_bytes{mountpoint!="/media"})
      # System uptime in days
      - record: node_uptime_days
        expr: (time() - node_boot_time_seconds) / (24*60*60)
      # File handles usage percent
      - record: node_filefd_usage_percent
        expr: 100*node_filefd_allocated/node_filefd_maximum
      # Inode usage percent
      - record: node_filesystem_inodes_usage_percent
        expr: 100-100*(node_filesystem_files_free / node_filesystem_files)
      ##
      ## Alert rules that monitor the health of a host using metrics exported by Node Exporter.
      ##
      # Page if a host is down and not responding to scrapes.
      - alert: HostDown
        expr: up{job="node"} == 0
        for: 5m
        labels:
          severity: p0
        annotations:
          description: "{{`Haruspex failed to scrape {{ $labels.instance }} for more than 5m. This indicates that the host is likely down or in an unhealthy state.`}}"
          summary: "{{`Host {{ $labels.instance }} is down`}}"
      # Page if a host is flapping between up and down states.
      - alert: HostFlapping
        expr: changes(up{job="node"}[1h]) > 5
        labels:
          severity: p0
        annotations:
          description: "{{`Availability status of {{ $labels.instance }} changed {{ $value }} times over the last 1 hour. This indicates that the host is likely in an unhealthy state.`}}"
          summary: "{{`Host {{ $labels.instance }} is flapping`}}"
      # Page if disk usage is very high.
      - alert: HostHighDiskUsage
        expr: node_filesystem_usage_percent >= 95
        labels:
          severity: p0
        annotations:
          summary: "{{`Filesystem mounted at {{ $labels.mountpoint }} on host {{ $labels.instance }} is running out of space`}}"
          description: "{{`The filesystem mounted at {{ $labels.mountpoint }} and backed by the {{ $labels.device }} device is currently at {{ $value }}% usage and is very close to running out of space.`}}"
      # Page if the disk is predicted to fill up in 4 hours.
      - alert: HostDiskWillFillIn4Hours
        expr: predict_linear(node_filesystem_usage_percent[1h], 60 * 60 * 4) >= 100
        for: 10m
        labels:
          severity: p0
        annotations:
          description: "{{`At current rate of usage the filesystem mounted on {{ $labels.mountpoint }} for host {{ $labels.instance }} is predicted to fill up within the next 4 hours.`}}"
          summary: "{{`Filesystem mounted on {{ $labels.mountpoint }} is predicted to fill up within the next 4 hours for host {{ $labels.instance }}`}}"
      # Page if the number of inodes available in a filesystem is at or near depletion.
      - alert: HostHighInodeUsage
        expr: node_filesystem_inodes_usage_percent > 95
        labels:
          severity: p0
        annotations:
          summary: "{{`High Inode usage on {{ $labels.mountpoint }} for {{ $labels.instance }}`}}"
          description: "{{`Inode usage percent on {{ $labels.instance }} for the filesystem mounted on {{ $labels.mountpoint }} is {{ $value }}% which at or near depletion. This indicates that there is an abnormally large number of files stored on the filesystem, usually caused by one or more applications creating many files without cleaning them up afterwards.`}}"
      # Page if the number of inodes available in a filesystem is predicted to be depleted within the next 4 hours.
      - alert: HostInodesDepletedIn4Hours
        expr: predict_linear(node_filesystem_inodes_usage_percent[1h], 60 * 60 * 4) >= 100
        labels:
          severity: p0
        annotations:
          summary: "{{`Inodes available on {{ $labels.mountpoint }} for {{ $labels.instance }} is predicted to be depleted in 4 hours`}}"
          description: "{{`At current rate of usage the number of available Inodes on host {{ $labels.instance }} for the filesystem mounted on {{ $labels.mountpoint }} is predicted to be depleted within the next 4 hours. This indicates that there is an abnormally large number of files stored on the filesystem, usually caused by one or more applications creating many files without cleaning them up afterwards.`}}"
      # Page if the number of open files is very high and approaching the system limit.
      - alert: HostOpenFilesLimit
        expr: node_filefd_usage_percent > 95
        labels:
          severity: p0
        annotations:
          summary: "{{`File handles on {{ $labels.instance }} depleted`}}"
          description: "{{`File handles usage on {{ $labels.instance }} is currently {{ $value }}% which is at or near depletion. This indicates that one or more processes have a large number of files open.`}}"
      # Page if the open files limit will be depleted within the next 4 hours.
      - alert: HostOpenFilesDepletedIn4Hours
        expr: predict_linear(node_filefd_usage_percent[1h], 60 * 60 * 4) >= 100
        labels:
          severity: p0
        annotations:
          summary: "{{`File handles usage on {{ $labels.instance }} is predicted to be depleted in 4 hours`}}"
          description: "{{`At current rate of usage the number of available file handles on host {{ $labels.instance }} is predicted to be depleted within the next 4 hours. This indicates that one or more processes have a large number of files open.`}}"
      # Alert if the host is experiencing memory pressure.
      - alert: HostMemoryPressure
        expr: node_memory_available_percent <= 5 and rate(node_vmstat_pgmajfault[5m]) > 1024
        for: 10m
        labels:
          severity: p0
        annotations:
          summary: "{{`Memory pressure on host {{ $labels.instance }}`}}"
          description: "{{`There is only {{ $value }}% amount of available memory for application use on host {{ $labels.instance }} and a high rate of major page faults. Hosts that have memory pressure will experience stability issues caused by the kernel OOM killer and degraded performance from active pages being paged out from main memory to disk if a swap space is configured.`}}"
      # Alert if the system clock is unsynchronized.
      - alert: HostUnsynchronizedClock
        expr: node_ntp_stratum == 16
        for: 5m
        labels:
          severity: p1
        annotations:
          summary: "{{`Clock unsynchronized on {{ $labels.instance }}`}}"
          description: "{{`The system clock on {{ $labels.instance }} is not synchronized with an NTP server.`}}"
      # Alert if the NTP sanity check fails.
      - alert: HostNTPSanity
        expr: node_ntp_sanity == 0
        for: 5m
        labels:
          severity: p1
        annotations:
          summary: "{{`NTP sanity check failed on {{ $labels.instance }}`}}"
          description: "{{`The NTP sanity check run on {{ $labels.instance }} failed. This indicates that there is a large offset between the local clock and configured NTP server(s), or the root synchronization distance with the NTP server(s) is excessively high. In both cases this means that the local clock time is likely inaccurate and different NTP server(s) should be used.`}}"
      # Alert if the textfile collector encounters errors.
      - alert: HostTextfileCollectorErrors
        expr: node_textfile_scrape_error > 0
        for: 5m
        labels:
          severity: p1
        annotations:
          summary: "{{`Errors encountered when running textfile collector on {{ $labels.instance }}`}}"
          description: "{{`One or more errors occurred while opening or reading textfile(s) on {{ $labels.instance }}. This indicates that some metrics being placed into the textfile(s) are not being exported and scraped by Prometheus as expected.`}}"
      # Page if the host has a low amount of entropy.
      - alert: HostLowEntropy
        expr: node_entropy_available_bits <= 256
        for: 10m
        labels:
          severity: p1
        annotations:
          summary: "{{`Low entropy on host {{ $labels.instance }}`}}"
          description: "{{`The number of bits available in the entropy pool over the last 10 minutes has remained below {{ $value }} which is very low. A depleted entropy pool will cause applications that read from /dev/random to block. Consider installing and configuring haveged to inject more entropy into the pool.`}}"
      # Alert if the disk is predicted to fill up in 2 weeks.
      - alert: HostDiskWillFillIn2Weeks
        expr: predict_linear(node_filesystem_usage_percent{mountpoint!="/tmp"}[7d], 60 * 60 * 24 * 14) >= 100
        for: 10m
        labels:
          severity: p1
        annotations:
          description: "{{`At current rate of usage the filesystem mounted on {{ $labels.mountpoint }} for host {{ $labels.instance }} is predicted to fill up within the next 2 weeks.`}}"
          summary: "{{`Filesystem mounted on {{ $labels.mountpoint }} is predicted to fill up within the next 2 weeks for host {{ $labels.instance }}`}}"
      # Alert if the host swap space usage is high.
      - alert: HostHighSwapUsage
        expr: 100*node_memory_SwapFree_bytes/node_memory_SwapTotal_bytes < 10
        for: 10m
        labels:
          severity: p2
        annotations:
          summary: "{{`Swap usage on {{ $labels.instance }} is high`}}"
          description: "{{`There is only {{ $value }}% free swap space remaining on {{ $labels.instance }}.`}}"
      # Alert if the host is experiencing a large amount of paging activity.
      - alert: HostPagingActivity
        expr: rate(node_vmstat_pgpgout{}[5m]) > 100000
        for: 10m
        labels:
          severity: p2
        annotations:
          summary: "{{`Paging activity on {{ $labels.instance }}`}}"
          description: "{{`There are {{ $value }} pages/s being paged out to disk on {{ $labels.instance }} which is very high. This indicates that the host may be experiencing memory pressure.`}}"
      # Alert if the host has not been rebooted in a long time.
      - alert: HostNeverRebooted
        expr: node_uptime_days > 180
        labels:
          severity: p2
        annotations:
          summary: "{{`Host {{ $labels.instance }} has not been rebooted in {{ $value }} days`}}"
          description: "{{`Host {{ $labels.instance }} has not been rebooted in {{ $value }} days which likely indicates that the currently installed and running kernel is out of date.`}}"
      # Alert if amount of memory allocated to the page cache is very low.
      - alert: HostPageCacheSize
        expr: node_uptime_days > 7 and (node_memory_Cached_bytes/node_memory_MemTotal_bytes) < 0.10
        for: 10m
        labels:
          severity: p2
        annotations:
          summary: "{{`Memory allocated to page cache on {{ $labels.instance }} is low`}}"
          description: "{{`Less than 10% of total physical memory is allocated to the page cache on {{ $labels.instance }}. This indicates that the host is potentially oversubscribed and that applications are not able to take full advantage of the I/O performance benefits associated with page caching.`}}"
      # Alert if CPU usage is excessively high for an extended period of time.
      - alert: HostHighCPUUsage
        expr: 100-sum(node_cpu_usage_percent{mode=~"idle|iowait"}) by (instance) > 85
        for: 2h
        labels:
          severity: p2
        annotations:
          summary: "{{`High CPU usage on {{ $labels.instance }}`}}"
          description: "{{`CPU usage on {{ $labels.instance }} has remained over 85% over the past 2 hours which is excessively high.`}}"
      # Alert if the host is retransmitting TCP segments.
      - alert: HostRetransmittedTCPSegments
        expr: rate(node_netstat_Tcp_RetransSegs{}[5m]) > 100
        for: 30m
        labels:
          severity: p2
        annotations:
          summary: "{{`Retransmitted TCP segments detected on {{ $labels.instance }}`}}"
          description: "{{`Retransmitted TCP segments detected on {{ $labels.instance }} which is an indicator of saturation and other network issues which can adversely impact performance.`}}"
      # Alert if the host is dropping packets.
      - alert: HostDroppedPackets
        expr: rate(node_network_receive_drop_total{}[5m]) > 100 or rate(node_network_transmit_drop_total{}[5m]) > 100
        for: 30m
        labels:
          severity: p2
        annotations:
          summary: "{{`Dropped packets detected on {{ $labels.instance }}`}}"
          description: "{{`Dropped packets detected on {{ $labels.instance }} over device {{ $labels.device }} which is an indicator of saturation and other network issues that can adversely impact network performance.`}}"
  # Optionally discover a list of monitored hosts through Skylab service discovery.
  # If results are discovered, a default set of system baseline alerts are generated
  # using metrics that are pushed into Haruspex by a palantir metrics agent such as Caelumd.
  # In most cases this option is unnecessary and should only be used if an environment
  # is unable to deploy and use node exporter for system level monitoring.
  monitored_hosts: '{{ discovered.metrics-agent.hostnames }}'
  prometheus:
    rule_files:
      - '{{ service_home }}/var/conf/caelumd_alerts.yml'
      - '{{ service_home }}/var/conf/baseline_rules.yml'
      - '{{ service_home }}/var/conf/user_defined_rules.yml'
      - '{{ discovered.rulemanager-alert-rules-file }}'
    address: '{{ host.hostname }}'
    alertmanager_path: '{{ discovered.alertmanager-path }}'
    alertmanager_scheme: '{{ discovered.alertmanager-scheme }}'
    alertmanagers: '{{ discovered.all-alertmanagers }}'
    context_path: '/prometheus'
    evaluation_interval: 1m
    hostname: '{{ host.hostname }}'
    custom_configs: '{{ exclude }}'
    custom_scrapes: '{{ exclude }}'
    port: 9090
    protocol: https
    certificate_file: '{{ ssl.cert_path }}'
    key_file: '{{ ssl.pem_path }}'
    ca_file: '{{ ssl.ca_path }}'
    prometheus_metrics: '{{ discovered.prometheus-metrics }}'
    scrape_interval: 30s
    scrape_timeout: 30s
    alert_relabel_configs:
      # Add hostname label when sending alert to alertmanager for inhibition purposes.
      - regex: "^(.*):\\d+$"
        source_labels:
          - instance
        target_label: host

discovery:
  consumes:
    alertmanager-path:
      role: alertmanager
      select: path
    alertmanager-scheme:
      role: alertmanager
      select: scheme
    all-alertmanagers:
      all: true
      role: alertmanager
      select: endpoint
    haruspex-sink:
      role: haruspex-sink
    metrics-agent:
      all: false
      role: metrics-agent
      stack: '*'
    prometheus-metrics:
      all: true
      role: prometheus-metrics
      stack: '*'
    prometheus-alerts:
      all: true
      role: prometheus-alerts
      stack: '*'
    rulemanager-alert-rules-file:
      role: rulemanager
      select: alert_rules_file
  produces:
    prometheus:
      external_hostname: '{{ conf.external_hostname }}'
      path: '{{ conf.prometheus.context_path }}'
      port: '{{ conf.prometheus.port }}'
      role: prometheus
      scheme: '{{ conf.prometheus.protocol }}'
      multipass-creds:
        id: '{{conf.oauth.client_id}}'
        secret: '{{conf.oauth.client_secret}}'
    prometheus-api:
      external_hostname: '{{ conf.external_hostname }}'
      path: '{{ conf.prometheus.context_path }}/api/v1'
      port: '{{ conf.prometheus.port }}'
      role: prometheus-api
      scheme: '{{ conf.prometheus.protocol }}'
    prometheus-metrics:
      job_name: prometheus
      metrics_path: '{{ conf.prometheus.context_path }}/metrics'
      port: '{{ conf.prometheus.port }}'
      role: prometheus-metrics
      scheme: '{{ conf.prometheus.protocol }}'
    prometheus-push:
      external_hostname: '{{ conf.external_hostname }}'
      path: '{{ conf.prometheus.context_path }}/push'
      port: '{{ self_discovered.prometheus.port }}'
      role: prometheus-push
      scheme: '{{ self_discovered.prometheus.scheme }}'
    multipass-oauth-client:
      client:
        authorizedGrantTypes:
          - authorization_code
        id: '{{conf.oauth.client_id}}'
        redirectUris:
          - '{{ self_discovered.prometheus.external_uri }}'
        secret: '{{conf.oauth.client_secret}}'
      role: multipass-oauth-client
managed_files:
  var/conf/prometheus.yml:
    live-reload: kick
    type: tmpl
  var/conf/baseline_rules.yml:
    live-reload: kick
    content: baseline_rules
    type: yaml
  var/conf/user_defined_rules.yml:
    live-reload: kick
    content: user_defined_rules
    type: yaml
  var/conf/caelumd_alerts.yml:
    live-reload: kick
    type: tmpl
  var/conf/logrotate.conf:
    live-reload: kick
    type: tmpl
generated_secrets:
  oauth-secret:
    length: 24
    secret-type: random_string
