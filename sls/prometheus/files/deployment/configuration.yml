default_conf:
  ENV:
    CONFIG_FILE: '{{ service_home }}/var/conf/prometheus.yml'
    LOG_FILE: '{{ service_home }}/var/log/prometheus-console.log'
    LOG_LEVEL: info
    RELOAD_URL: '{{ conf.prometheus.protocol }}://{{ conf.prometheus.hostname }}:{{ conf.prometheus.port }}/-/reload'
    STORAGE_TSDB_PATH: '{{ service_home }}/var/data/'
    WEB_CA_FILE: '{{ conf.prometheus.ca_file }}'
    WEB_CERTIFICATE_FILE: '{{ conf.prometheus.certificate_file }}'
    WEB_EXTERNAL_URL: '{{ self_discovered.prometheus.external_uri }}'
    WEB_KEY_FILE: '{{ conf.prometheus.key_file }}'
    WEB_LISTEN_ADDRESS: '{{ conf.prometheus.address }}:{{ conf.prometheus.port }}'
    WEB_PROTOCOL: '{{ conf.prometheus.protocol }}'
    WEB_ROUTE_PREFIX: '{{ conf.prometheus.context_path }}'
  oauth:
    client_id: '{{stack_name}}-{{service_name}}'
    client_secret: "{{plaintext_secret('oauth-secret')}}"
  baseline_rules:
    groups:
    ##
    ## The set of SLS compliant service baseline alerting rules evaluated by Prometheus.
    ##
    - name: sls.rules
      rules: []
    ##
    ## The set of system baseline rules evaluated by Prometheus.
    ##
    - name: host.rules
      rules:
      ##
      ## Recording rules that pre-compute frequently needed and computationally expensive expressions.
      ##
      # Active CPU usage percent (100-%idle)
      - record: node_cpu_active_usage_percent
        expr: 100-irate(node_cpu_seconds_total{job="node",mode="idle"}[5m])*100
      # IOWait CPU usage percent
      - record: node_cpu_iowait_usage_percent
        expr: irate(node_cpu_seconds_total{job="node",mode="iowait"}[5m])*100
      # System CPU usage percent
      - record: node_cpu_system_usage_percent
        expr: irate(node_cpu_seconds_total{job="node",mode="system"}[5m])*100
      # User CPU usage percent
      - record: node_cpu_user_usage_percent
        expr: irate(node_cpu_seconds_total{job="node",mode="user"}[5m])*100
      # Memory percent available for application use
      - record: node_memory_available_percent
        expr: 100*node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes
      # Filesystem space usage percent
      - record: node_filesystem_usage_percent
        expr: 100*(1-node_filesystem_avail_bytes{mountpoint!="/media"}/node_filesystem_size_bytes{mountpoint!="/media"})
      # System uptime in days
      - record: node_uptime_days
        expr: (time() - node_boot_time_seconds) / (24*60*60)
      # File handles usage percent
      - record: node_filefd_usage_percent
        expr: 100*node_filefd_allocated/node_filefd_maximum
      ##
      ## Alert rules that monitor the health of a host using metrics exported by Node Exporter.
      ##
      # Page if a host is down and not responding to scrapes.
      - alert: HostDown
        expr: up{job="node"} == 0
        for: 5m
        labels:
          severity: p0
        annotations:
          description: "{{`Haruspex failed to scrape {{ $labels.instance }} for more than 5m. This indicates that the host is likely down or in an unhealthy state.`}}"
          summary: "{{`Host {{ $labels.instance }} is down`}}"
      # Page if a host is flapping between up and down states.
      - alert: HostFlapping
        expr: changes(up{job="node"}[1h]) > 5
        labels:
          severity: p0
        annotations:
          description: "{{`Availablility status of {{ $labels.instance }} changed {{ $value }} times over the last 1 hour. This indicates that the host is likely in an unhealthy state.`}}"
          summary: "{{`Host {{ $labels.instance }} is flapping`}}"
      # Page if disk usage is very high.
      - alert: HostHighDiskUsage
        expr: node_filesystem_usage_percent >= 95
        labels:
          severity: p0
        annotations:
          summary: "{{`Filesystem mounted at {{ $labels.mountpoint }} on host {{ $labels.instance }} is running out of space`}}"
          description: "{{`The filesystem mounted at {{ $labels.mountpoint }} and backed by the {{ $labels.device }} device is currently at {{ $value }}% usage and is at risk of running out of space soon.`}}"
      # Page if the disk is predicted to fill up in 4 hours.
      - alert: HostDiskWillFillIn4Hours
        expr: predict_linear(node_filesystem_usage_percent[1h], 60 * 60 * 4) >= 100
        for: 10m
        labels:
          severity: p0
        annotations:
          description: "{{`At current rate of usage the filesystem {{ $labels.mountpoint }} on host {{ $labels.instance }} is predicted to fill up within the next 4 hours.`}}"
          summary: "{{`Filesystem mounted at {{ $labels.mountpoint }} is predicted to fill up within the next 4 hours on {{ $labels.instance }}`}}"
      # Alert if the disk is predicted to fill up in 2 weeks.
      - alert: HostDiskWillFillIn2Weeks
        expr: predict_linear(node_filesystem_usage_percent{path!~"/tmp"}[7d], 60 * 60 * 24 * 14) >= 100
        for: 10m
        labels:
          severity: p1
        annotations:
          description: "{{`At current rate of usage the disk {{ $labels.path }} on host {{ $labels.instance }} is predicted to fill up within the next 2 weeks.`}}"
          summary: "{{`Disk {{ $labels.path }} predicted to fill up within the next 2 weeks on {{ $labels.instance }}`}}"
      # Page if the host is experiencing memory pressure.
      - alert: HostMemoryPressure
        expr: min_over_time(node_memory_available_percent{}[5m]) <= 10
        for: 10m
        labels:
          severity: p0
        annotations:
          summary: "{{`Memory pressure on host {{ $labels.instance }}`}}"
          description: "{{`There is only {{ $value }}% amount of available memory for application use on host {{ $labels.host }}. Hosts that run out of memory will experience stability issues caused by the kernel OOM killer and degraded performance from active pages being paged out from main memory to disk if a swap device is configured.`}}"
      # Page if the host has a low amount of entropy.
      - alert: HostLowEntropy
        expr: avg_over_time(node_entropy_available_bits{}[10m]) <= 1024
        for: 10m
        labels:
          severity: p0
        annotations:
          summary: "{{`Low entropy on host {{ $labels.instance }}`}}"
          description: "{{`The average number of bits available in the entropy pool over the last 10 minutes is {{ $value }} which is very low.`}}"
      # Alert if the host has not been rebooted in a long time.
      - alert: HostNeverRebooted
        expr: node_uptime_days > 180
        labels:
          severity: p2
        annotations:
          summary: "{{`Host {{ $labels.instance }} has not been rebooted in {{ $value }} days`}}"
          description: "{{`Host {{ $labels.instance }} has not been rebooted in {{ $value }} days which likely indicates that the currently installed and running kernel is out of date.`}}"
      # Page if the number of open files is very high and approaching the system limit.
      - alert: HostOpenFilesLimit
        expr: node_filefd_usage_percent > 95
        labels:
          severity: p0
        annotations:
          summary: "{{`File handles on {{ $labels.instance }} depleted`}}"
          description: "{{`File handles usage on {{ $labels.instance }} is currently at {{ $value }}% which is at or near depletion.`}}"
      # Page if the open files limit will be depleted within the next 4 hours.
      - alert: HostOpenFilesDepletedIn4Hours
        expr: predict_linear(node_filefd_usage_percent[1h], 60 * 60 * 4) >= 100
        labels:
          severity: p0
        annotations:
          summary: "{{`File handles on {{ $labels.instance }} predicted to be depleted in 4 hours`}}"
          description: "{{`At current rate of usage the pool of available file handles on host {{ $labels.instance }} is predicted to be depleted within the next 4 hours.`}}"
  # Optionally discover a list of monitored hosts through Skylab service discovery.
  # If results are discovered, a default set of system baseline alerts are generated
  # using metrics that are pushed into Haruspex by a palantir metrics agent such as Caelumd.
  # In most cases this option is unnecessary and should only be used if an environment
  # is unable to deploy and use node exporter for system level monitoring.
  monitored_hosts: '{{ discovered.metrics-agent.hostnames }}'
  prometheus:
    rule_files:
      - '{{ service_home }}/var/conf/caelumd_alerts.yml'
      - '{{ service_home }}/var/conf/baseline_rules.yml'
      - '{{ discovered.rulemanager-alert-rules-file }}'
    address: '{{ host.hostname }}'
    alertmanager_path: '{{ discovered.alertmanager-path }}'
    alertmanager_scheme: '{{ discovered.alertmanager-scheme }}'
    alertmanagers: '{{ discovered.all-alertmanagers }}'
    context_path: '/prometheus'
    evaluation_interval: 1m
    hostname: '{{ host.hostname }}'
    custom_configs: '{{ exclude }}'
    custom_scrapes: '{{ exclude }}'
    port: 9090
    protocol: https
    certificate_file: '{{ ssl.cert_path }}'
    key_file: '{{ ssl.pem_path }}'
    ca_file: '{{ ssl.ca_path }}'
    prometheus_metrics: '{{ discovered.prometheus-metrics }}'
    scrape_interval: 30s
    scrape_timeout: 30s

discovery:
  consumes:
    alertmanager-path:
      role: alertmanager
      select: path
    alertmanager-scheme:
      role: alertmanager
      select: scheme
    all-alertmanagers:
      all: true
      role: alertmanager
      select: endpoint
    metrics-agent:
      all: false
      role: metrics-agent
      stack: '*'
    prometheus-metrics:
      all: true
      role: prometheus-metrics
      stack: '*'
    prometheus-alerts:
      all: true
      role: prometheus-alerts
      stack: '*'
    rulemanager-alert-rules-file:
      role: rulemanager
      select: alert_rules_file
  produces:
    prometheus:
      external_hostname: '{{ conf.external_hostname }}'
      path: '{{ conf.prometheus.context_path }}'
      port: '{{ conf.prometheus.port }}'
      role: prometheus
      scheme: '{{ conf.prometheus.protocol }}'
      multipass-creds:
        id: '{{conf.oauth.client_id}}'
        secret: '{{conf.oauth.client_secret}}'
    prometheus-api:
      external_hostname: '{{ conf.external_hostname }}'
      path: '{{ conf.prometheus.context_path }}/api/v1'
      port: '{{ conf.prometheus.port }}'
      role: prometheus-api
      scheme: '{{ conf.prometheus.protocol }}'
    prometheus-metrics:
      job_name: prometheus
      metrics_path: '{{ conf.prometheus.context_path }}/metrics'
      port: '{{ conf.prometheus.port }}'
      role: prometheus-metrics
      scheme: '{{ conf.prometheus.protocol }}'
    prometheus-push:
      external_hostname: '{{ conf.external_hostname }}'
      path: '{{ conf.prometheus.context_path }}/push'
      port: '{{ self_discovered.prometheus.port }}'
      role: prometheus-push
      scheme: '{{ self_discovered.prometheus.scheme }}'
    multipass-oauth-client:
      client:
        authorizedGrantTypes:
          - authorization_code
        id: '{{conf.oauth.client_id}}'
        redirectUris:
          - '{{ self_discovered.prometheus.external_uri }}'
        secret: '{{conf.oauth.client_secret}}'
      role: multipass-oauth-client
    # The configuration used for rotating logs generated by Prometheus.
    logrotate:
      filepath: '{{ conf.ENV.LOG_FILE }}'
      role: logrotate
      settings:
        - compress
        - copytruncate
        - hourly
        - rotate 7
        - size 512M
managed_files:
  var/conf/prometheus.yml:
    live-reload: kick
    type: tmpl
  var/conf/baseline_rules.yml:
    live-reload: kick
    content: baseline_rules
    type: yaml
  var/conf/caelumd_alerts.yml:
    live-reload: kick
    type: tmpl
generated_secrets:
  oauth-secret:
    length: 24
    secret-type: random_string
